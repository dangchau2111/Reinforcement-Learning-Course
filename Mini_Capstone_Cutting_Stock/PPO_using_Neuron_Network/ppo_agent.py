import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import gc
from collections import deque

class PPOAgent:
    def __init__(self, env, policy, gamma=0.99, lr=0.00005, batch_size=32, epochs=10, clip_epsilon=0.2):
        self.env = env
        self.policy = policy
        self.gamma = gamma
        self.lr = lr
        self.batch_size = batch_size
        self.epochs = epochs
        self.clip_epsilon = clip_epsilon
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.memory = deque(maxlen=5000)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.lr)

    def store_memory(self, state, action, log_probs, value, reward, done):
        """LÆ°u tráº£i nghiá»‡m vÃ o bá»™ nhá»› vá»›i kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh"""
        # if isinstance(state, torch.Tensor):
        # state = state.to(self.device)  # Chuyá»ƒn tá»« Tensor -> Numpy Ä‘á»ƒ xá»­ lÃ½
        
        # if len(state.shape) == 3:  # Náº¿u cÃ³ batch size, loáº¡i bá» nÃ³
        #     state = state.squeeze(0)

        # padded_state = self.env.pad_stock(state)  # Äáº£m báº£o Ä‘áº§u vÃ o há»£p lá»‡
        self.memory.append((state, action, log_probs, value, reward, done))



    def choose_action(self, state):
        """Chá»n hÃ nh Ä‘á»™ng báº±ng chÃ­nh sÃ¡ch PPO"""
        state = torch.tensor(state, dtype=torch.float32).clone().detach().unsqueeze(0).to(self.device) # ÄÆ°a state lÃªn tensor xá»­ lÃ½ báº±ng GPU vÃ  thÃªm chiá»u batch
        
        with torch.no_grad(): # Náº¿u khÃ´ng cáº§n tÃ­nh Ä‘áº¡o hÃ m, sá»­ dá»¥ng torch.no_grad() Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ tÃ­nh toÃ¡n
            stock_dist, x_dist, y_dist, rotate_dist, value = self.policy(state)     # Tráº£ vá» phÃ¢n phá»‘i xÃ¡c suáº¥t vÃ  giÃ¡ trá»‹ Æ°á»›c lÆ°á»£ng

        # Chá»n 1 hÃ nh Ä‘á»™ng ngáº«u nhiÃªn tá»« phÃ¢n phá»‘i xÃ¡c suáº¥t
        action = (stock_dist.sample(), x_dist.sample(), y_dist.sample(), rotate_dist.sample())

        # TÃ­nh log xÃ¡c suáº¥t cá»§a hÃ nh Ä‘á»™ng Ä‘Æ°á»£c chá»n (PPO sá»­ dá»¥ng hÃ m loss dá»±a trÃªn log-probability Ä‘á»ƒ so sÃ¡nh xÃ¡c suáº¥t cÅ© vÃ  má»›i cá»§a hÃ nh Ä‘á»™ng.)
        log_probs = sum(dist.log_prob(act) for dist, act in zip([stock_dist, x_dist, y_dist, rotate_dist], action))

        # Tráº£ vá» tuple dáº¡ng sá»‘ nguyÃªn cá»§a action vd: (3, 45, 78, 1) Ä‘á»ƒ dÃ¹ng trong mÃ´i trÆ°á»ng gym
        return tuple(a.item() for a in action), log_probs, value

    def train(self):
        """Huáº¥n luyá»‡n mÃ´ hÃ¬nh báº±ng thuáº­t toÃ¡n PPO"""
        if len(self.memory) < self.batch_size:
            return  # Chá» Ä‘áº¿n khi cÃ³ Ä‘á»§ batch Ä‘á»ƒ train

        batch = random.sample(self.memory, self.batch_size)
        states, actions, old_log_probs, values, rewards, dones = zip(*batch)

        # ğŸ“Œ Chuyá»ƒn dá»¯ liá»‡u sang tensor
        states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(self.device)
        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32, device=self.device)
        values = torch.tensor(values, dtype=torch.float32, device=self.device)
        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)
        dones = torch.tensor(dones, dtype=torch.float32, device=self.device)

        # ğŸ“Œ Chuyá»ƒn actions thÃ nh tensor, Ä‘áº£m báº£o cÃ¹ng kÃ­ch thÆ°á»›c
        actions = torch.stack([torch.tensor(a, dtype=torch.long) for a in actions]).to(self.device)
        
        # ğŸ”¥ Giá»›i háº¡n giÃ¡ trá»‹ `actions` Ä‘á»ƒ trÃ¡nh lá»—i pháº¡m vi ğŸ”¥
        actions[:, 0] = actions[:, 0].clamp(0, self.env.num_stocks - 1)  # stock_idx
        actions[:, 1] = actions[:, 1].clamp(0, self.env.max_stock_size[0] - 1)  # x pháº£i náº±m trong kÃ­ch thÆ°á»›c phÃ´i
        actions[:, 2] = actions[:, 2].clamp(0, self.env.max_stock_size[1] - 1)  # y pháº£i náº±m trong kÃ­ch thÆ°á»›c phÃ´i
        actions[:, 3] = actions[:, 3].clamp(0, 1)   # rotate pháº£i náº±m trong [0, 1]

        # ğŸ“Œ TÃ­nh returns báº±ng GAE
        returns = torch.zeros_like(rewards, device=self.device)
        discounted_sum = 0
        for i in reversed(range(len(rewards))):
            if dones[i]:
                discounted_sum = 0
            discounted_sum = rewards[i] + self.gamma * discounted_sum
            returns[i] = discounted_sum

        advantage = (returns - values).detach()

        for _ in range(self.epochs):
            stock_dist, x_dist, y_dist, rotate_dist, new_values = self.policy(states)

            # ğŸ“Œ Äáº£m báº£o `actions` Ä‘Ãºng kiá»ƒu dá»¯ liá»‡u trÆ°á»›c khi log_prob()
            new_log_probs = (
                stock_dist.log_prob(actions[:, 0]) +
                x_dist.log_prob(actions[:, 1]) +
                y_dist.log_prob(actions[:, 2]) +
                rotate_dist.log_prob(actions[:, 3])
            )

            # ğŸ“Œ TÃ­nh tá»· lá»‡ má»›i / cÅ©
            ratio = (new_log_probs - old_log_probs).exp()

            # ğŸ“Œ Clipped loss
            surrogate1 = ratio * advantage
            surrogate2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantage
            actor_loss = -torch.min(surrogate1, surrogate2).mean()
            critic_loss = (returns - new_values).pow(2).mean()

            # ğŸ“Œ Tá»•ng loss cá»§a PPO
            loss = actor_loss + 0.5 * critic_loss

            # ğŸ“Œ Cáº­p nháº­t trá»ng sá»‘
            self.policy.optimizer.zero_grad()
            loss.backward()
            self.policy.optimizer.step()

        # ğŸ“Œ Giáº£i phÃ³ng bá»™ nhá»›
        self.memory.clear()
        torch.cuda.empty_cache()
        import gc
        gc.collect()


