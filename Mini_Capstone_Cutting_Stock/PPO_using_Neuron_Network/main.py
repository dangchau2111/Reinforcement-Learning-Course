import torch
import numpy as np
import os
import matplotlib.pyplot as plt
from cutting_glass_env import CuttingGlassEnv
from ppo_policy import PPOPolicy
from ppo_agent import PPOAgent
import gc

# Cho ph√©p x·ª≠ l√Ω nhi·ªÅu th∆∞ vi·ªán OpenMP, tr√°nh l·ªói duplicate
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Gi·∫£i ph√≥ng b·ªô nh·ªõ tr√™n GPU tr∆∞·ªõc khi ch·∫°y (n·∫øu c√≥ GPU)
torch.cuda.empty_cache()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Using device: {device}")

# T·∫°o c√°c th∆∞ m·ª•c l∆∞u m√¥ h√¨nh v√† bi·ªÉu ƒë·ªì loss n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs("Model", exist_ok=True)
os.makedirs("Loss_Plots", exist_ok=True)

# C·∫•u h√¨nh t·∫≠p d·ªØ li·ªáu c·∫ßn d√πng (c√≥ th·ªÉ thay ƒë·ªïi dataset_id n·∫øu c·∫ßn)
dataset_id = 1

# Kh·ªüi t·∫°o m√¥i tr∆∞·ªùng v√† c√°c ƒë·ªëi t∆∞·ª£ng m√¥ h√¨nh, agent PPO
env = CuttingGlassEnv(dataset=dataset_id, csv_path="data_custom.csv")
policy = PPOPolicy(num_stocks=env.num_stocks, num_products=env.num_products).to(device)
agent = PPOAgent(env, policy)

# Thi·∫øt l·∫≠p s·ªë l∆∞·ª£ng episode hu·∫•n luy·ªán, kho·∫£ng c√°ch l∆∞u m√¥ h√¨nh v√† danh s√°ch l∆∞u l·∫°i reward c·ªßa c√°c episode
num_episodes = 10000
save_interval = 1000
rewards_history = []

# H√†m v·∫Ω bi·ªÉu ƒë·ªì loss v√† reward, sau ƒë√≥ l∆∞u v√†o file ·∫£nh
def save_loss_plot(actor_losses, critic_losses, rewards_history, episode):
    plt.figure(figsize=(15, 5))
    
    # V·∫Ω bi·ªÉu ƒë·ªì Actor Loss
    plt.subplot(1, 3, 1)
    plt.plot(actor_losses, label="Actor Loss", color='blue')
    plt.xlabel("Training Step")
    plt.ylabel("Loss")
    plt.title("Actor Loss")
    plt.legend()
    plt.grid()
    
    # V·∫Ω bi·ªÉu ƒë·ªì Critic Loss
    plt.subplot(1, 3, 2)
    plt.plot(critic_losses, label="Critic Loss", color='green')
    plt.xlabel("Training Step")
    plt.ylabel("Loss")
    plt.title("Critic Loss")
    plt.legend()
    plt.grid()
    
    # V·∫Ω bi·ªÉu ƒë·ªì Reward qua th·ªùi gian
    plt.subplot(1, 3, 3)
    plt.plot(rewards_history, label="Total Reward", color='red')
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.title("Reward Over Time")
    plt.legend()
    plt.grid()
    
    # L∆∞u bi·ªÉu ƒë·ªì v√†o th∆∞ m·ª•c Loss_Plots v·ªõi t√™n file bao g·ªìm episode
    loss_plot_path = f"Loss_Plots/loss_plot_ep{episode}.png"
    plt.savefig(loss_plot_path)
    plt.close()
    print(f"üìä Loss plot saved at {loss_plot_path}")

# V√≤ng l·∫∑p hu·∫•n luy·ªán qua c√°c episode
for episode in range(1, num_episodes + 1):
    # Reset m√¥i tr∆∞·ªùng t·∫°i ƒë·∫ßu m·ªói episode
    state = env.reset()
    # Chuy·ªÉn ƒë·ªïi state t·ª´ numpy v·ªÅ tensor, th√™m chi·ªÅu batch (unsqueeze(0))
    state = {key: torch.tensor(value, dtype=torch.float32).unsqueeze(0).to(device) for key, value in state.items()}
    done = False
    total_reward = 0

    # V√≤ng l·∫∑p t∆∞∆°ng t√°c trong m·ªôt episode cho ƒë·∫øn khi k·∫øt th√∫c (done=True)
    while not done:
        # Ch·ªçn h√†nh ƒë·ªông t·ª´ agent d·ª±a tr√™n state hi·ªán t·∫°i
        action, log_probs, value = agent.choose_action(state)
        # Th·ª±c hi·ªán h√†nh ƒë·ªông trong m√¥i tr∆∞·ªùng v√† nh·∫≠n k·∫øt qu·∫£ (next_state, reward, done, info)
        next_state, reward, done, _ = env.step(action)
        # Chuy·ªÉn ƒë·ªïi next_state t·ª´ numpy sang tensor, th√™m chi·ªÅu batch
        for key, val in next_state.items():
            try:
                tensor_val = torch.tensor(val, dtype=torch.float32)
                next_state[key] = tensor_val.unsqueeze(0).to(device)
            except Exception as e:
                print(f" L·ªói khi chuy·ªÉn ƒë·ªïi {key}: {e}")
                print(f" Gi√° tr·ªã c·ªßa {key}: {val}")
        # L∆∞u l·∫°i tr·∫£i nghi·ªám v√†o b·ªô nh·ªõ c·ªßa agent
        agent.store_memory(state, action, log_probs, value, reward, done)
        # C·∫≠p nh·∫≠t state hi·ªán t·∫°i th√†nh next_state cho b∆∞·ªõc l·∫∑p ti·∫øp theo
        state = next_state
        total_reward += reward
    
    # D·ªçn d·∫πp b·ªô nh·ªõ v√† gi·∫£i ph√≥ng t√†i nguy√™n sau m·ªói episode
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.set_per_process_memory_fraction(0.8, 0)
    
    # Sau m·ªói 10 episode, ti·∫øn h√†nh training cho policy
    if episode % 10 == 0:
        agent.train()
        print(f"Model training at episode {episode}")
    # Sau m·ªói 200 episode, x√≥a b·ªô nh·ªõ c·ªßa agent (c√≥ th·ªÉ gi√∫p tr√°nh memory overflow)
    if episode % 200 == 0:
        agent.memory.clear()
        print(f"Memory cleared at episode {episode}")
    
    # L∆∞u l·∫°i t·ªïng reward c·ªßa episode v√†o danh s√°ch l·ªãch s·ª≠
    rewards_history.append(total_reward)
    print(f" Episode {episode}/{num_episodes},  Total Reward: {total_reward}")
    
    # L∆∞u model v√† bi·ªÉu ƒë·ªì loss sau m·ªói save_interval episode
    if episode % save_interval == 0:
        model_path = f"Model/ppo_policy_ep{episode}.pth"
        torch.save(policy, model_path)
        print(f" Model saved at {model_path}")
        save_loss_plot(agent.actor_losses, agent.critic_losses, rewards_history, episode)
